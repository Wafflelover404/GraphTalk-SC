# llm.py - Интеграция языковой модели

## Обзор
Обеспечивает генерацию ответов на основе ИИ с использованием языковых моделей для создания человекочитаемых ответов из результатов поиска в базе знаний. Этот модуль служит мостом между необработанными данными БЗ и ответами на естественном языке.

## Основные функции
- **Интеграция с GPT-4**: Использует GPT-4o-mini для экономичных ответов ИИ
- **Контекстно-зависимый**: Объединяет запросы пользователей с результатами поиска в БЗ
- **Без веб-поиска**: Фокусируется исключительно на содержимом базы знаний
- **Простой интерфейс**: Одна функция для вызовов LLM

## Основная функция

### `llm_call(message, data)`
Генерирует человекочитаемые ответы, используя LLM на основе пользовательского запроса и данных базы знаний.

#### Параметры
- `message` (str): Исходный вопрос или запрос пользователя
- `data` (str): Результаты поиска в базе знаний как контекст

#### Возвращает
- `str`: ИИ-сгенерированный ответ, который объединяет данные БЗ с объяснением на естественном языке

#### Стратегия системного промпта
Функция использует трехчастную структуру промпта:
1. **Системные инструкции**: Определяет роль ИИ как помощника БЗ
2. **Контекст базы знаний**: Предоставляет результаты поиска как справочные данные
3. **Пользовательский запрос**: Фактический пользовательский запрос для ответа

## Детали реализации

### Конфигурация модели
- **Модель**: `gpt-4o-mini` (экономичный вариант GPT-4)
- **Провайдер**: клиент g4f (GPT4Free)
- **Веб-поиск**: Отключен (фокусируется только на предоставленных данных БЗ)
- **Температура**: По умолчанию (сбалансированная креативность/точность)

### Структура сообщения
```python
messages = [
    {
        "role": "system", 
        "content": "Инструкции - {system_prompt}"
    },
    {
        "role": "system", 
        "content": "Поисковый запрос базы знаний - {data}"
    },
    {
        "role": "user", 
        "content": "запрос пользователя - {message}"
    }
]
```

## Примеры использования

### Базовая генерация ответов
```python
from llm import llm_call

# Результаты поиска в БЗ
kb_data = [
    "Keynode: ostis_technology",
    "Link Content: OSTIS is a technology for building knowledge bases",
    "Link Content: Semantic networks are used in OSTIS"
]

# Генерация человеческого ответа
user_question = "Что такое технология OSTIS?"
response = llm_call(user_question, "\\n".join(kb_data))

print(response)
# Вывод: "Технология OSTIS - это фреймворк для построения баз знаний, использующий семантические сети..."
```

### Интеграция с поиском
```python
from sc_search import kb_search
from llm import llm_call

# Полный рабочий процесс
query = "Как работают семантические сети?"

# 1. Поиск в базе знаний
kb_results = kb_search(query)

# 2. Генерация человеческого ответа
if kb_results:
    context = "\\n".join(kb_results)
    human_response = llm_call(query, context)
    print(human_response)
else:
    print("В базе знаний информация не найдена")
```

### Интеграция с API
```python
# В api.py
@app.post("/query")
async def process_query(request: QueryRequest, humanize: bool = False):
    kb_results = kb_search(request.text)
    
    if humanize:
        context = "\\n".join(kb_results) if isinstance(kb_results, list) else kb_results
        response = llm_call(request.text, context)
    else:
        response = kb_results
        
    return APIResponse(status="success", response=response)
```

## Анализ системного промпта

Системный промпт инструктирует ИИ:
1. **Действовать как помощник БЗ**: Фокусироваться на содержимом базы знаний
2. **Анализировать предоставленные данные**: Извлекать релевантную информацию из результатов поиска
3. **Предоставлять полезные ответы**: Генерировать краткие, релевантные ответы
4. **Рассуждать на основе контекста**: Основывать ответы на предоставленных данных БЗ

```
"Вы - ИИ-помощник. Вы получаете набор данных из базы знаний. 
Ваша задача - проанализировать эти данные и предоставить полезный, краткий ответ на основе сообщения пользователя. 
Сосредоточьтесь на извлечении релевантной информации и рассуждениях на основе данных базы знаний в контексте запроса пользователя."
```

## Функции качества ответов

### Интеграция контекста
- Объединяет несколько результатов поиска БЗ в связные объяснения
- Поддерживает фокус на содержимом базы знаний
- Избегает галлюцинаций, основывая ответы на предоставленных данных

### Обработка естественного языка
- Преобразует технические идентификаторы БЗ в читаемые объяснения
- Структурирует информацию логично для человеческого понимания
- Предоставляет рассуждения и связи между концепциями

### Краткость
- Генерирует сфокусированные ответы без ненужных подробностей
- Балансирует полноту с читаемостью
- Адаптирует длину ответа к доступным данным БЗ

## Обработка ошибок

### Проблемы с подключением
```python
def safe_llm_call(message, data):
    try:
        return llm_call(message, data)
    except Exception as e:
        return f"Ошибка генерации ответа: {str(e)}"
```

### Обработка пустых данных
```python
# В интеграции с API
if not kb_results or not any(kb_results):
    return "В базе знаний релевантная информация не найдена"
    
context = "\\n".join(kb_results)
if not context.strip():
    return "Поиск в базе знаний вернул пустые результаты"
    
response = llm_call(query, context)
```

## Соображения производительности

### Лимиты API
- Клиент g4f может иметь ограничения скорости
- Рассмотрите реализацию логики повторов для производства
- Мониторинг использования API для управления затратами

### Время ответа
- Вызовы LLM добавляют задержку к ответам поиска
- Рассмотрите кэширование для частых запросов
- Реализуйте обработку тайм-аутов

### Контроль качества
- Ответы зависят от качества данных БЗ
- Лучшие результаты поиска приводят к лучшим ответам LLM
- Рассмотрите валидацию ответов для критических приложений

## Варианты конфигурации

### Выбор модели
```python
# Альтернативные модели (изменить в llm_call)
response = client.chat.completions.create(
    model="gpt-4",          # Высшее качество, дороже
    model="gpt-3.5-turbo",  # Быстрее, дешевле
    model="gpt-4o-mini",    # Текущий по умолчанию
    # ... остальные параметры
)
```

### Контроль температуры
```python
# Добавить параметр температуры для контроля креативности
response = client.chat.completions.create(
    model="gpt-4o-mini",
    temperature=0.7,  # 0.0 = детерминистический, 1.0 = креативный
    # ... остальные параметры
)
```

## Паттерны интеграции

### Простая гуманизация
```python
# Прямое преобразование БЗ в человеческий вид
raw_results = kb_search("запрос")
human_response = llm_call("запрос", "\\n".join(raw_results))
```

### Многоэтапная обработка
```python
# Сложный рабочий процесс с валидацией
raw_results = kb_search("запрос")
if validate_kb_results(raw_results):
    processed_context = preprocess_kb_data(raw_results)
    response = llm_call("запрос", processed_context)
    return postprocess_response(response)
```

### Стратегия резервного варианта
```python
# Мягкая деградация
try:
    human_response = llm_call(query, kb_data)
    return human_response
except Exception:
    return kb_data  # Вернуть сырые результаты, если LLM не работает
```

## Лучшие практики

1. **Качество данных**: Убедитесь, что результаты поиска БЗ релевантны и чисты
2. **Размер контекста**: Мониторьте лимиты токенов для больших наборов результатов БЗ
3. **Обработка ошибок**: Реализуйте резервные варианты для сбоев LLM
4. **Управление затратами**: Мониторьте использование и затраты API
5. **Валидация ответов**: Проверяйте качество и релевантность ответов
6. **Кэширование**: Кэшируйте ответы для повторяющихся запросов, когда это уместно
